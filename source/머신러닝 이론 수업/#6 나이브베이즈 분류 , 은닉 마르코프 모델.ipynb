{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이즈 분류\n",
    "## 나이브 베이즈 분류 개요\n",
    "### 나이브 베이즈 개요\n",
    "* 매개 변수 x,y가 있을때\n",
    "    * 그룹 A에 속할 확률 : P1(x,y)\n",
    "    * 그룹 B에 속할 확률 : P2(x,y)\n",
    "    * P1(x,y) > P2(x,y)이변 해당 데이터는 P1 그룹에 속함\n",
    "* 베이즈 정리에 의해, 분류하려는 대상의 각 확률을 측정함\n",
    "* 확률이 큰 부분으로 분류하는 방법\n",
    "    * 데이터에 대해 Naive(소박하게, 전문적이지 않게) 판단함\n",
    "\n",
    "### 나이브베이즈 특징\n",
    "* 지도학습 환경에서 효율적으로 사용할 수 있음\n",
    "* 분류를 위한 학습 데이터의 양이 매우 적음\n",
    "    * 머신러닝을 수행하기 위한 학습 데이터가 많지 않아도 됨\n",
    "* 간단한 구조\n",
    "* 가정이 단순\n",
    "* 복잡한 실제 상황에서 잘 동작함\n",
    "\n",
    "### 조건부 확률 \n",
    "* P(A|B)=B가 발생했을때, A일 확률\n",
    "* 새로운 메일이 스팸 메일일 확률 구하는 식\n",
    "    * 사후 확률(posterior probability) : B가 발생했을때, A일 확률\n",
    "    * 사전 확률(prior probability) : A일 확률\n",
    "    * 우도(likelihood) : B가 이전 A에서 사용되었을 확률\n",
    "    * 주변 우도(marginal likelihood) : 모든 곳에서 B가 나타날 확률\n",
    "#### P(A|B) = P(B|A)P(A) / P(B) = P(A∩B) / P(B)\n",
    "\n",
    "### Laplace Smoothing\n",
    "* 만약, 기존에 없는 새로운 단어가 들어온다면?\n",
    "    * 해당 단어에 대한 확률이 0이기 떄문에, 모든 확률은 0이 됨\n",
    "* 이러한 문제를 해결하기 위해\n",
    "    * 각 분자에 +1을 함\n",
    "    * 분모에는 모든 데이터의 수(중복 제거)를 더해줌\n",
    "    \n",
    "### 언더플로우 방지\n",
    "* 스팸 메일을 분류하기 위한 단어가 매우 많고,'광고'라는 단어는 몇 번 나오지 않았다면?\n",
    "    * P(광고|스팸메일)가 너무 작아지면서 0에 거의 가까워 질 수 있음\n",
    "* 확률에 Log를 취하여 언더플로우를 방지함\n",
    "\n",
    "####  Log(P(A|B)) = Log(P(B|A)P(B))\n",
    "\n",
    "## 스팸메일 판단하기\n",
    "\n",
    "### 데이터 준비\n",
    "* 기존의 이메일의 분류 내용과 포함되었던 단어 정리\n",
    "* 메일 / 단어 / 분류\n",
    "    * 메일1 / 친구,점수,학교,학교 / 정상\n",
    "    * 메일2 / 긴급,광고,도착 / 스팸\n",
    "    * 메일3 / 점수,할인,긴급,친구,친구 / 정상\n",
    "    * 메일4 / 광고,도착,도착,친구 / 스팸\n",
    "    * 메일5 / 할인,긴급,도착,학교 / 스팸\n",
    "* 새로운 메일에 포함된 단어가 친구,광고,긴급일 경우 스팸 or 정상메일\n",
    "\n",
    "### 스팸 메일 판단수행\n",
    "\n",
    "* 각 단어의 빈도 수 계산\n",
    "    * Count(긴급,정상) = 1\n",
    "    * Count(광고,정상) = 0\n",
    "    * Count(친구,정상) = 3\n",
    "    * Count(긴급,스팸) = 2\n",
    "    * Count(광고,스팸) = 2\n",
    "    * Count(친구,스팸) = 1\n",
    "    \n",
    "### 스팸메일 판단 수행\n",
    "* 각 항목의 확률 계산\n",
    "    * P(긴급|정상) = 1/9 , P(광고|정상) = 0/9 , P(친구|정상) = 3/9\n",
    "* 전체 메일에서 정상 메일과 스팸 메일의 확률 계산\n",
    "    * P(정상) = 정상메일(2) / 전체메일(5) = 2/5 , P(스팸) = 스팸메일(3) / 전체메일(5) = 3/5\n",
    "    * P(정상| words) = {(1/9)*(0/9)*(3/9)*(2/5)}=0\n",
    "\n",
    "### 스팸 메일 판단수행\n",
    "* 단어가 스팸 메일일 확률을 구하면 \n",
    "    * P(정상| words) = {(2/11)*(2/11)*(1/11)*(3/5)}=0.0018\n",
    "* P(정상| words) < P(스팸|words)이므로, 해당 메일은 스팸 메일로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 은닉 마르코프 모델\n",
    "## 은닉 마르코프 모델 개요\n",
    "### 마르코프 모델이란?\n",
    "* 어떠한 시점에서 N의 가능한 상태 S={S1,S2,S3,~SN)를 확인\n",
    "* 일정한 시간 간격으로 어떠한 상태로 발전\n",
    "* 상태간의 발전이 확률적으로 표현 되어짐\n",
    "* 마르코프 가정\n",
    "    * 시간 t에서 상태는 오직 가장 최근 r개 데이터에만 의존함\n",
    "\n",
    "### 에르고딕(Ergodic)모델\n",
    "* 상태가 옮겨지는 상황에 따라 두 가지 모델로 구분\n",
    "* 에르고딕 모델은 어떤 상태로 돌아간 후에도 다시 뒤로 돌아갈 수 있음\n",
    "\n",
    "### 좌우(Left-to-Right)모델\n",
    "* 어떤 상태로 들어가면 뒤로 돌아갈 수 없음\n",
    "* 날씨 에서는 사용이 불가능 함\n",
    "    * 맑음, 흐림과 같은 상태가 향후에도 다시 나타날 수 있음\n",
    "* 음성 인식이나 필기 인식 등과 같이, 어떤 패턴이 지나간 후에 다시 나타나지 않을 때 사용\n",
    "\n",
    "### 은닉 마르코프 모델\n",
    "* Hidden Markov Models의 약자\n",
    "* 마르코프 모델에서 숨겨진 부분 추정\n",
    "\n",
    "### 은닉 마르코프 모델의 파라미터\n",
    "* N : 모델의 상태 수\n",
    "* M : 상태에서 관측 가능한 심볼의 수\n",
    "* A : 상태 전이 확률 분포 (aij : i 상태에서 j 상태로의 전이 확률\n",
    "* B : 관측 심볼 확률 분포 (bi(j) : 상태 i 에서 j가 관측될 확률)\n",
    "* ㅠ : 초기 상태 확률 분포 (N개의 상태들이 처음에 나타날 확률)\n",
    "\n",
    "## 은닉 마르코프 모델의 한계점 및 활용\n",
    "\n",
    "### 평가(Evaluation)\n",
    "* 관측한 관측열이 나올 수 있는 확률 문제\n",
    "    * 날씨를 관측할 때, 관측 값이 하나가 아니고 여러 개라면?\n",
    "    * 주어진 모델에서 관측열이 출력될 확률을 효과적으로 계산할 수 있어야 함\n",
    "* 확률값에 따라 최적읭 모델을 다수의 모델로 부터 인식할 수 있음\n",
    "* 전향(Forward)과 후향(Backword)알고리즘을 활용하여 해결\n",
    "    * 동적 프로그래밍으로 중복 계산 제거\n",
    "    * 전방 계산에 의해 확률 평가\n",
    "\n",
    "### 최적 상태열(Optimal State Sequence) 찾기\n",
    "* 가장 최적의 숨겨져 있는 상태열을 어떻게 찾아낼 것인가?\n",
    "* 모든 상태열을 계산하고 그 중 최적을 찾는 것은 엄청난 계산이 발생\n",
    "* 비터비(Viterbi) 알고리즘으로 해결\n",
    "    * 동적 프로그래밍으로 전방 계산을 수행\n",
    "    * 최적 경로를 역추적\n",
    "\n",
    "### 파라미터 추정(Parameter Estimation)\n",
    "* 우도(likelihood)를 최대화 하는 모델의 각 파라미터 추정은?\n",
    "    * 관측열을 가장 잘 설명하는 모델의 파라미터들을 어떻게 최적화하여 구할것 인가 ?\n",
    "    * 학습의 문제라고도 함(성능을 결정하는 중요한 사항)\n",
    "* 바움-웰치 재추정 알고리즘으로 해결\n",
    "    * 모델의 변수를 모르는 경우에 이를 추정하는 방법\n",
    "    * 초기에 랜덤하게 혹은 어떤 다른 방법으로 모델 변수를 설정한 상태에서, 관측데이터(X)가 이 모델(lambda_0)로 부터 생성되엇을 확률을 계산\n",
    "    * 모델을 조금 수정후(lambda_1), 확률 계산\n",
    "    * 그 중 큰 확률을 결정하면서 계속 반복\n",
    "    \n",
    "### 은닉 마르코프 모델의 활용\n",
    "* 필기 인식, 음성 명령 인식, 제스쳐 인식, 물체 트래킹\n",
    "* 주가 예측, 날씨 예측, 교통상황 예측 등에 활용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
