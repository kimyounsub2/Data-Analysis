{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습\n",
    "## 강화학습 개요\n",
    "### 강화학습 개념\n",
    "* 학습에 의해 답이 정해지는 것이 아님\n",
    "* 모르는 환경에서 보상 값(Reward)이 최대가 되도록 행동함\n",
    "\n",
    "### Trial and Error\n",
    "* 환경과의 상호작용으로 학습하는 것\n",
    "* 해보지 않고 예측하지 않음\n",
    "* 무엇인가를 수행 하면서 조정함\n",
    "    * 공부를 열심히 하면 엄마가 용돈을 준다\n",
    "    * 대회에서 1등을 하면 상금을 준다\n",
    "* 어떻게 하면 더 많은 보상(Reward)을 받을지 노력하게 됨\n",
    "* 기존 Reward에 따라 앞으로의 행동이 결정됨\n",
    "\n",
    "### Delayed Reward\n",
    "* 시간의 개념이 포함됨\n",
    "* 시간의 순서가 있는 문제를 풀어냄\n",
    "* 지금의 행동이 바로 Reward로 이어질 것인가?\n",
    "* 지금의 행동이 향후 더 큰 Reward로 이어질 것인가?\n",
    "* 다른 행동과 합해져서 더 큰 Reward가 될 것인가?\n",
    "\n",
    "### 강화학습의 역사\n",
    "* 동물의 행동에 대한 심리학 연구에서 출발\n",
    "* 스키너 상자 실험\n",
    "    * 굶긴 비둘기를 상자에 넣음\n",
    "    * 한쪽의 원판을 쪼면 먹이통에 먹이가 나오게 설치\n",
    "    * 비둘기는 다양한 행동을 함\n",
    "    * 우연히 원판을 쪼게 되면 먹이가 나오는 것을 알게 됨\n",
    "    * 그 이후에는 비둘기는 원판을 쪼는 반응을 계속하여 먹이를 먹게됨\n",
    "\n",
    "## 강화학습 수행 절차\n",
    "### MDP 개념\n",
    "* MDP는 로봇이 보석을 얻기 위해 어떻게 할 지 학습하는 것\n",
    "* MDP의 구성\n",
    "    * State : 로봇의 위치\n",
    "    * Action : 앞/뒤/좌/우로 이동하는 행동\n",
    "    * Reward : 로봇이 가지려는 보석\n",
    "\n",
    "### Value Function\n",
    "* State - value Function\n",
    "    * 어떠한 상태 S에 대한 가치\n",
    "    * 다음으로 이동할 수 있는 상태들의 가치를 보고, 높은 가치의 상태로 이동\n",
    "    * 이동할 상태의 Value Function을 구하는 것이 중요한 문제\n",
    "* Action-value function\n",
    "    * 다음 상태로 가기 위해서는 어떻게 해야 하는지 알아야 함\n",
    "    * 어떤 상태s에서 행동 a를 취할 경우 받을 기대값\n",
    "    * 다른 말로 Q-value라고 불리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공신경망\n",
    "## 인공신경망 개요\n",
    "### 신경망과 인공신경망\n",
    "* 신경망이란?\n",
    "    * 인간의 뇌 구조\n",
    "    * 신경세포(Neuron) : 정보처리의 단위\n",
    "    * 여러 개의 뉴런을 병렬처리 함으로써 인간은 빠르게 기능을 수행함\n",
    "    * 복잡하고, 비선형적이고, 병렬적인 처리 기능\n",
    "    \n",
    "* 인공신경망이란?\n",
    "    * 인간의 뇌를 부분적으로 흉내 낸것\n",
    "\n",
    "### 인공신경망의 모델링\n",
    "* 각 신호값을 가중치와 곱한 값들의 합과 뉴런이 가지는 한계치와 비교\n",
    "    * 한계치를 넘어서면 1\n",
    "    * 넘지 않으면 -1을 다음 노드로 전달\n",
    "    * 인공신경망의 학습\n",
    "        * 반복적인 조정으로 학습을 실시함\n",
    "\n",
    "### 활성화 함수 개념\n",
    "* 결과값을 내보낼 때 사용하는 함수\n",
    "* 전이 함수라고도 부름\n",
    "* 다음과 같은 함수로 구성\n",
    "    * 계단함수\n",
    "    * 부호함수\n",
    "    * 시그모이드 함수\n",
    "    * 선형 함수\n",
    "    * 쌍곡 탄젠트 함수\n",
    "\n",
    "## 인공신경망 수행 방법\n",
    "### 퍼셉트론 개요\n",
    "* 계단 함수 또는 부호 함수를 사용하여 만들어진 단순한 뉴런\n",
    "* 퍼셉트론에서는 초평면과 선형 분리 개념이 적용\n",
    "    * 초평면 : N차원 공간을 두 개의 영역으로 나눈 평면\n",
    "    * 선형 분리 : 값의 분포를 2개로 나눠지는 평면이 존재하면 선형 분리 가능\n",
    "* 선형 분리가 가능해야 퍼셉드론 표현 가능\n",
    "\n",
    "### 다층 피드 포워드 신경망 개요\n",
    "* Layered Feed-Forward neural network의 약자로 LFF라고 불림\n",
    "* 퍼셉트론에의 문제 해결\n",
    "    * 선형 분리가 불가능한 문제는 퍼셉트론에서 사용 불가\n",
    "* LFF는 여러 개의 직선으로 층을 나누어서 문제 해결\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
